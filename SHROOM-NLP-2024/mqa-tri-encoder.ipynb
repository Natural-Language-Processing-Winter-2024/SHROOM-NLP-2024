{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import Tokenizer\n",
    "import warnings\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import wandb\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def casual_mask(size):\n",
    "    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HallucinationDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, seq_len=512) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "#         self.pad_token = torch.tensor([tokenizer.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "        self.label_mapping = {'Not Hallucination': 1, 'Hallucination': 0}\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        task = self.dataset.iloc[idx]['task']\n",
    "        hypothesis = self.dataset.iloc[idx]['hyp']\n",
    "        context = self.dataset.iloc[idx]['src'] if task == 'PG' else self.dataset.iloc[idx]['tgt']\n",
    "        label = self.dataset.iloc[idx]['label']\n",
    "        \n",
    "#         hypothesis_encoded = self.tokenizer.encode(hypothesis).ids\n",
    "#         context_encoded = self.tokenizer.encode(context).ids\n",
    "        hypothesis_encoding = self.tokenizer(hypothesis, truncation=True, padding='max_length', max_length=self.seq_len)\n",
    "        context_encoding = self.tokenizer(context, truncation=True, padding='max_length', max_length=self.seq_len)\n",
    "        \n",
    "#         for _ in range(len(hypothesis_encoded), self.seq_len): hypothesis_encoded.append(self.pad_token)\n",
    "#         for _ in range(len(context_encoded), self.seq_len): context_encoded.append(self.pad_token)\n",
    "        \n",
    "        if isinstance(label, str):\n",
    "            label_encoded = self.label_mapping[label]\n",
    "        else: label_encoded = label\n",
    "        \n",
    "#         hypothesis_encoded = torch.tensor(hypothesis_encoded)\n",
    "#         context_encoded = torch.tensor(context_encoded)\n",
    "        \n",
    "        return {\n",
    "#             \"hypothesis_encoded\": hypothesis_encoded,\n",
    "#             \"context_encoded\": context_encoded,\n",
    "            \"hypothesis_encoded\": torch.tensor(hypothesis_encoding['input_ids']),\n",
    "            \"context_encoded\": torch.tensor(context_encoding['input_ids']),\n",
    "#             \"hypothesis_mask\": (hypothesis_encoded != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
    "#             \"context_mask\": (context_encoded != self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(self.seq_len),\n",
    "            \"hypothesis_mask\": torch.tensor(hypothesis_encoding['attention_mask']).unsqueeze(0).unsqueeze(0),\n",
    "            \"context_mask\": torch.tensor(context_encoding['attention_mask']).unsqueeze(0).unsqueeze(0),\n",
    "            \"label\": torch.tensor(label_encoded),\n",
    "            \"hypothesis\": hypothesis,\n",
    "            \"context\": context\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/SHROOM_unlabeled-training-data-v2/train.model-agnostic_labeled.csv')\n",
    "train_data.fillna('', inplace=True)\n",
    "\n",
    "val_data = pd.read_json(\"data/SHROOM_dev-v2/val.model-agnostic.json\")\n",
    "val_data.fillna('', inplace=True)\n",
    "\n",
    "test_data = pd.read_json(\"data/SHROOM_test-labeled/test.model-agnostic.json\")\n",
    "test_data.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_all_sentences(ds):\n",
    "    for i in range(len(ds)):\n",
    "        x = ds.iloc[i]['ref']\n",
    "        yield ds.iloc[i]['hyp'] + ' ' + ds.iloc[i][x if x != 'either' else 'tgt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_tokenizer(ds):\n",
    "    tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "    tokenizer.train_from_iterator(get_all_sentences(ds), trainer=trainer)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer = build_tokenizer(train_data)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = HallucinationDataset(train_data, tokenizer)\n",
    "val_dataset = HallucinationDataset(val_data, tokenizer)\n",
    "test_dataset = HallucinationDataset(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, eps: float = 10 ** -6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features))  # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(features))  # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "        # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim=-1, keepdim=True)  # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim=-1, keepdim=True)  # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)  # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)  # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)  # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0)  # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)  # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiQueryAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, groups: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model  # Embedding vector size\n",
    "        self.h = h  # Number of heads\n",
    "        self.groups = groups\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "        # Make sure that number of groups is divisible by h\n",
    "        # assert h % groups == 0, \"h should be divisible by groups\"\n",
    "\n",
    "        self.d_k = d_model // h  # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)  # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model // self.groups, bias=False)  # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model // self.groups, bias=False)  # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False)  # Wo\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1)  # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q)  # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k)  # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v)  # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h // self.groups, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h // self.groups, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        queries = torch.split(query, split_size_or_sections=self.h // self.groups, dim=1)\n",
    "        \n",
    "        lst_x, lst_attn = [], []\n",
    "        \n",
    "        for query in queries:\n",
    "            x, attn = MultiQueryAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "            lst_x.append(x)\n",
    "            lst_attn.append(attn)\n",
    "        \n",
    "        x, attention_scores = torch.cat(lst_x, dim=1), torch.cat(lst_attn, dim=1)\n",
    "\n",
    "        # # Calculate attention\n",
    "        # x, self.attention_scores = MultiQueryAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiQueryAttentionBlock,\n",
    "                 feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MixEncoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, features: int, self_attention_block: MultiQueryAttentionBlock,\n",
    "                 feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, x_mask, y, y_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(y, x, x, y_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MixEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, hypothesis, hypothesis_mask, context, context_mask):\n",
    "        for layer in self.layers:\n",
    "            hypothesis = layer(hypothesis, hypothesis_mask, context, context_mask)\n",
    "        return self.norm(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 d_model: int,\n",
    "                 dropout: float,\n",
    "                 hypothesis_encoder: Encoder, \n",
    "                 context_encoder: Encoder, \n",
    "                 mix_encoder: MixEncoder, \n",
    "                 hypothesis_embed: InputEmbeddings, \n",
    "                 context_embed: InputEmbeddings,\n",
    "                 hypothesis_pos: PositionalEncoding, \n",
    "                 context_pos: PositionalEncoding) -> None:\n",
    "        super().__init__()\n",
    "        self.hypothesis_encoder = hypothesis_encoder\n",
    "        self.context_encoder = context_encoder\n",
    "        self.mix_encoder = mix_encoder\n",
    "        self.hypothesis_embed = hypothesis_embed\n",
    "        self.context_embed = context_embed\n",
    "        self.hypothesis_pos = hypothesis_pos\n",
    "        self.context_pos = context_pos\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=512, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n",
    "        ])\n",
    "        \n",
    "        self.fc = nn.Linear(16, 2)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.hypothesis_norm = LayerNormalization(d_model)\n",
    "        self.context_norm = LayerNormalization(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def hypothesis_encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.hypothesis_embed(src)\n",
    "        src = self.hypothesis_pos(src)\n",
    "        return self.hypothesis_encoder(src, src_mask)\n",
    "    \n",
    "    def context_encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.context_embed(src)\n",
    "        src = self.context_pos(src)\n",
    "        return self.context_encoder(src, src_mask)\n",
    "    \n",
    "    def hypothesis_context_mix(self, hypothesis_output, hypothesis_mask, context_output, context_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        output = self.mix_encoder(hypothesis_output, hypothesis_mask, context_output, context_mask)\n",
    "    \n",
    "        output += self.dropout(self.hypothesis_norm(hypothesis_output))\n",
    "        output += self.dropout(self.context_norm(context_output))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def project(self, outputs):\n",
    "        outputs = outputs.permute(0, 2, 1)\n",
    "        for conv_layer in self.conv_layers:\n",
    "            outputs = F.relu(conv_layer(outputs))\n",
    "        pooled_output = F.max_pool1d(outputs, outputs.size(2)).squeeze(2)\n",
    "        return self.activation(self.fc(pooled_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size: int, seq_len: int, d_model: int = 512, num_groups: int = 1, num_layers: int = 12, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
    "    hypothesis_embed = InputEmbeddings(d_model, vocab_size)\n",
    "    context_embed = InputEmbeddings(d_model, vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    hypothesis_pos = PositionalEncoding(d_model, seq_len, dropout)\n",
    "    context_pos = PositionalEncoding(d_model, seq_len, dropout)\n",
    "\n",
    "    # Create the encoder blocks\n",
    "    hypothesis_encoder_blocks = []\n",
    "    for _ in range(num_layers):\n",
    "        encoder_self_attention_block = MultiQueryAttentionBlock(d_model, h, num_groups, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        hypothesis_encoder_blocks.append(encoder_block)\n",
    "    \n",
    "    context_encoder_blocks = []\n",
    "    for _ in range(num_layers):\n",
    "        encoder_self_attention_block = MultiQueryAttentionBlock(d_model, h, num_groups, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        context_encoder_blocks.append(encoder_block)\n",
    "    \n",
    "    mix_encoder_blocks = []\n",
    "    for _ in range(num_layers):\n",
    "        mix_cross_attention_block = MultiQueryAttentionBlock(d_model, h, num_groups, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = MixEncoderBlock(d_model, mix_cross_attention_block, feed_forward_block, dropout)\n",
    "        mix_encoder_blocks.append(encoder_block)\n",
    "    \n",
    "    hypothesis_encoder = Encoder(d_model, nn.ModuleList(hypothesis_encoder_blocks))\n",
    "    context_encoder = Encoder(d_model, nn.ModuleList(context_encoder_blocks))\n",
    "    mix_encoder = MixEncoder(d_model, nn.ModuleList(mix_encoder_blocks))\n",
    "    \n",
    "    transformer = Transformer(d_model, dropout, hypothesis_encoder, context_encoder, mix_encoder, hypothesis_embed, context_embed, hypothesis_pos, context_pos)\n",
    "\n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return transformer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (hypothesis_encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x EncoderBlock(\n",
       "        (self_attention_block): MultiQueryAttentionBlock(\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward_block): FeedForwardBlock(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (residual_connections): ModuleList(\n",
       "          (0-1): 2 x ResidualConnection(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm): LayerNormalization()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNormalization()\n",
       "  )\n",
       "  (context_encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x EncoderBlock(\n",
       "        (self_attention_block): MultiQueryAttentionBlock(\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward_block): FeedForwardBlock(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (residual_connections): ModuleList(\n",
       "          (0-1): 2 x ResidualConnection(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm): LayerNormalization()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNormalization()\n",
       "  )\n",
       "  (mix_encoder): MixEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x MixEncoderBlock(\n",
       "        (self_attention_block): MultiQueryAttentionBlock(\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward_block): FeedForwardBlock(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (residual_connections): ModuleList(\n",
       "          (0-1): 2 x ResidualConnection(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm): LayerNormalization()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNormalization()\n",
       "  )\n",
       "  (hypothesis_embed): InputEmbeddings(\n",
       "    (embedding): Embedding(30522, 512)\n",
       "  )\n",
       "  (context_embed): InputEmbeddings(\n",
       "    (embedding): Embedding(30522, 512)\n",
       "  )\n",
       "  (hypothesis_pos): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (context_pos): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): Conv1d(512, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (2): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  )\n",
       "  (fc): Linear(in_features=16, out_features=2, bias=True)\n",
       "  (activation): Sigmoid()\n",
       "  (hypothesis_norm): LayerNormalization()\n",
       "  (context_norm): LayerNormalization()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = build_model(vocab_size=tokenizer.get_vocab_size(), seq_len=512)\n",
    "model = build_model(vocab_size=tokenizer.vocab_size, seq_len=512)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wandb.init(project=\"Hallucination Detection\", entity=\"subhanshu20135\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=10e-4, weight_decay=10e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00: 100%|██████████████████████████████████████████████████████████████████| 125/125 [00:55<00:00,  2.27it/s]\n",
      "Processing Epoch 00: 100%|████████████████████████████████████████████████████████████████████| 63/63 [00:09<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 7.340923507690429 Validation Loss: 9.251575912748065 Train Accuracy: 0.441 Validation Accuracy: 0.56312625250501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 01: 100%|██████████████████████████████████████████████████████████████████| 125/125 [00:51<00:00,  2.41it/s]\n",
      "Processing Epoch 01: 100%|████████████████████████████████████████████████████████████████████| 63/63 [00:09<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 7.337360883712768 Validation Loss: 9.25240833797152 Train Accuracy: 0.441 Validation Accuracy: 0.56312625250501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 02: 100%|██████████████████████████████████████████████████████████████████| 125/125 [00:52<00:00,  2.40it/s]\n",
      "Processing Epoch 02: 100%|████████████████████████████████████████████████████████████████████| 63/63 [00:09<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train Loss: 7.339178365707397 Validation Loss: 9.25240965495034 Train Accuracy: 0.441 Validation Accuracy: 0.56312625250501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 03: 100%|██████████████████████████████████████████████████████████████████| 125/125 [00:52<00:00,  2.40it/s]\n",
      "Processing Epoch 03: 100%|████████████████████████████████████████████████████████████████████| 63/63 [00:09<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train Loss: 7.337629810333252 Validation Loss: 9.236242619771806 Train Accuracy: 0.441 Validation Accuracy: 0.56312625250501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 04: 100%|██████████████████████████████████████████████████████████████████| 125/125 [00:51<00:00,  2.41it/s]\n",
      "Processing Epoch 04: 100%|████████████████████████████████████████████████████████████████████| 63/63 [00:09<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Train Loss: 7.339720308303833 Validation Loss: 9.250493772446163 Train Accuracy: 0.441 Validation Accuracy: 0.56312625250501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 05: 100%|██████████████████████████████████████████████████████████████████| 125/125 [00:51<00:00,  2.42it/s]\n",
      "Processing Epoch 05: 100%|████████████████████████████████████████████████████████████████████| 63/63 [00:09<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Train Loss: 7.3369019470214845 Validation Loss: 9.281496592930385 Train Accuracy: 0.441 Validation Accuracy: 0.56312625250501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 06: 100%|██████████████████████████████████████████████████████████████████| 125/125 [00:51<00:00,  2.41it/s]\n",
      "Processing Epoch 06: 100%|████████████████████████████████████████████████████████████████████| 63/63 [00:09<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Train Loss: 7.338115064620972 Validation Loss: 9.234732907915872 Train Accuracy: 0.441 Validation Accuracy: 0.56312625250501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 07: 100%|██████████████████████████████████████████████████████████████████| 125/125 [00:51<00:00,  2.42it/s]\n",
      "Processing Epoch 07: 100%|████████████████████████████████████████████████████████████████████| 63/63 [00:09<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Train Loss: 7.340805337905884 Validation Loss: 9.250510862895421 Train Accuracy: 0.441 Validation Accuracy: 0.56312625250501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 08: 100%|██████████████████████████████████████████████████████████████████| 125/125 [00:51<00:00,  2.40it/s]\n",
      "Processing Epoch 08: 100%|████████████████████████████████████████████████████████████████████| 63/63 [00:09<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Train Loss: 7.3395347442626955 Validation Loss: 9.267896769538758 Train Accuracy: 0.441 Validation Accuracy: 0.56312625250501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 09: 100%|██████████████████████████████████████████████████████████████████| 125/125 [00:51<00:00,  2.43it/s]\n",
      "Processing Epoch 09: 100%|████████████████████████████████████████████████████████████████████| 63/63 [00:09<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Train Loss: 7.343775835037231 Validation Loss: 9.249551190270317 Train Accuracy: 0.441 Validation Accuracy: 0.56312625250501\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    train_iterator = tqdm(train_loader, desc=f'Processing Epoch {epoch:02d}')\n",
    "    for batch in train_iterator:\n",
    "        hypothesis_encoded = batch[\"hypothesis_encoded\"].to(device)\n",
    "        context_encoded = batch[\"context_encoded\"].to(device)\n",
    "        \n",
    "        hypothesis_mask = batch[\"hypothesis_mask\"].to(device)\n",
    "        context_mask = batch[\"context_mask\"].to(device)\n",
    "        \n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        hypothesis_outputs = model.hypothesis_encode(hypothesis_encoded, hypothesis_mask)\n",
    "        \n",
    "        context_outputs = model.context_encode(context_encoded, context_mask)\n",
    "        \n",
    "        mix_outputs = model.hypothesis_context_mix(hypothesis_outputs, hypothesis_mask, context_outputs, context_mask)\n",
    "        \n",
    "        outputs = model.project(mix_outputs)\n",
    "        \n",
    "        predictions = torch.max(outputs, dim=1).values\n",
    "        \n",
    "        loss = criterion(predictions.float(), labels.float())\n",
    "\n",
    "        train_correct += (np.round(predictions.detach().cpu().numpy()) == labels.cpu().numpy()).sum().item()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        train_total += labels.size(0)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_spearmanr = 0.0\n",
    "    with torch.no_grad():\n",
    "        val_iterator = tqdm(val_loader, desc=f'Processing Epoch {epoch:02d}')\n",
    "        for batch in val_iterator:\n",
    "            hypothesis_encoded = batch[\"hypothesis_encoded\"].to(device)\n",
    "            context_encoded = batch[\"context_encoded\"].to(device)\n",
    "\n",
    "            hypothesis_mask = batch[\"hypothesis_mask\"].to(device)\n",
    "            context_mask = batch[\"context_mask\"].to(device)\n",
    "\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            hypothesis_outputs = model.hypothesis_encode(hypothesis_encoded, hypothesis_mask)\n",
    "\n",
    "            context_outputs = model.context_encode(context_encoded, context_mask)\n",
    "\n",
    "            mix_outputs = model.hypothesis_context_mix(hypothesis_outputs, hypothesis_mask, context_outputs, context_mask)\n",
    "\n",
    "            outputs = model.project(mix_outputs)\n",
    "\n",
    "            predictions = torch.max(outputs, dim=1).values\n",
    "\n",
    "            loss = criterion(predictions.float(), labels.float())\n",
    "\n",
    "            val_correct +=  (np.round(predictions.detach().cpu().numpy()) == labels.cpu().numpy()).sum().item()\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_total += labels.size(0)\n",
    "    \n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    train_accuracy = train_correct / train_total\n",
    "    val_accuracy = val_correct / val_total\n",
    "    \n",
    "    # wandb.log({\n",
    "    #     \"Train Loss\": train_loss,\n",
    "    #     \"Validation Loss\": val_loss,\n",
    "    #     \"Train Accuracy\": train_accuracy,\n",
    "    #     \"Validation Accuracy\": val_accuracy\n",
    "    # })\n",
    "    \n",
    "    torch.save(model.state_dict(), f\"weights/MQA_Model_{epoch}.pt\")\n",
    "    \n",
    "    print(\"Epoch:\", epoch, \"Train Loss:\", train_loss, \"Validation Loss:\", val_loss, \"Train Accuracy:\", train_accuracy, \"Validation Accuracy:\", val_accuracy)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████████████████████████████████████████████████████| 188/188 [00:28<00:00,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 9.830484556390884 Test Accuracy: 0.5926666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    test_iterator = tqdm(test_loader, desc=f'Processing: ')\n",
    "    for batch in test_iterator:\n",
    "        hypothesis_encoded = batch[\"hypothesis_encoded\"].to(device)\n",
    "        context_encoded = batch[\"context_encoded\"].to(device)\n",
    "\n",
    "        hypothesis_mask = batch[\"hypothesis_mask\"].to(device)\n",
    "        context_mask = batch[\"context_mask\"].to(device)\n",
    "\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        hypothesis_outputs = model.hypothesis_encode(hypothesis_encoded, hypothesis_mask)\n",
    "\n",
    "        context_outputs = model.context_encode(context_encoded, context_mask)\n",
    "\n",
    "        mix_outputs = model.hypothesis_context_mix(hypothesis_outputs, hypothesis_mask, context_outputs, context_mask)\n",
    "\n",
    "        outputs = model.project(mix_outputs)\n",
    "\n",
    "        predictions = torch.max(outputs, dim=1).values\n",
    "        \n",
    "        loss = criterion(predictions.float(), labels.float())\n",
    "\n",
    "        test_correct += (np.round(predictions.cpu().numpy()) == labels.cpu().numpy()).sum().item()\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "\n",
    "        test_total += labels.size(0)\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    \n",
    "    test_accuracy = test_correct / test_total\n",
    "\n",
    "    print(\"Test Loss:\", test_loss,\"Test Accuracy:\", test_accuracy)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4863820,
     "sourceId": 8208101,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
