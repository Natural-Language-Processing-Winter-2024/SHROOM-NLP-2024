{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"04b2b191f387469facbc7e0f63edd957":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c18583fabf94cf88d89e9d0ad83cd46","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_16ceb8ceabea4adeb2ed5d3c62a52e87","value":1}},"07bb3c8d23084467b680d0f8be879bcd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"08db236b9ee74ccb9ac456bf09e298e1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ea36c0ff6cd4559bf733fb73ff82693":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fca89659d3684477bb46613bbb96383d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_265b13864e334d2d8875d1de157c428a","value":1}},"16ceb8ceabea4adeb2ed5d3c62a52e87":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1c18583fabf94cf88d89e9d0ad83cd46":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"265b13864e334d2d8875d1de157c428a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2abefc6082af406ab1c955a880a2b419":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ae89d1a8a074a249b750d138587e44d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eb30e73c1e824fa8942f0c58104d696f","IPY_MODEL_df0a135d8a5b43d5ab94bef15b2db5aa","IPY_MODEL_a5e99c0d3739407799fde2f29a301d05"],"layout":"IPY_MODEL_fa5555299e2e47ae9d2cc7a7e58415f4"}},"2b25549d8eac4efd99bf1beb4fb26b0c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3e1566a3d2f64b5fbbaf7cc51b9c9902":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48be64dd9497468f83d73bd119591271":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cebd82bbc195424a908c9527ee1a21d3","placeholder":"​","style":"IPY_MODEL_8665cfefbc984fc4873e73cd96d6c018","value":"Downloading data files: 100%"}},"4f891d2316604dd08cd5ffd22c8854d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c70248a7e6e45199ed626fa68037174","placeholder":"​","style":"IPY_MODEL_07bb3c8d23084467b680d0f8be879bcd","value":"Generating val split: "}},"4facca9ecbd74aa5b4dc474634686064":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c70248a7e6e45199ed626fa68037174":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c4a2676871e492897d305d6d9a6fac9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"823cdbf0fa2c43559d01de4664258a86":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8665cfefbc984fc4873e73cd96d6c018":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86da540e05824f2c95b5c8bea9b4581d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d1f94d67f08449439e3191bcdf87c6bf","IPY_MODEL_cb886b4dac084c0290e1fd1c229b092e","IPY_MODEL_8b8fd80c79c54e479b15f798bc545b96"],"layout":"IPY_MODEL_3e1566a3d2f64b5fbbaf7cc51b9c9902"}},"8b8fd80c79c54e479b15f798bc545b96":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08db236b9ee74ccb9ac456bf09e298e1","placeholder":"​","style":"IPY_MODEL_977e8b1928ec42a285804dcc8fc13cb5","value":" 1/1 [00:00&lt;00:00,  1.86it/s]"}},"977e8b1928ec42a285804dcc8fc13cb5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f4e1bc76cfb4643877686a6f0271b52":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0ceffacff7f492d87084da291061006":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0f2fe09ab0a4a21acda513f96bb7faf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4f891d2316604dd08cd5ffd22c8854d9","IPY_MODEL_0ea36c0ff6cd4559bf733fb73ff82693","IPY_MODEL_de38e0a8f5a24cbdbf755db3cfd399ec"],"layout":"IPY_MODEL_9f4e1bc76cfb4643877686a6f0271b52"}},"a5e99c0d3739407799fde2f29a301d05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e35a5293e19748679095d1222f1a31e5","placeholder":"​","style":"IPY_MODEL_2abefc6082af406ab1c955a880a2b419","value":" 5.94G/5.94G [00:45&lt;00:00, 157MB/s]"}},"ac217ebd99d94729ac89ed81fc0a0ab5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aeaed97ed3f441e9aa2ce24c87e02d87":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af87959da48a436e842f58ac691717df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aff193ecfc2e4d5a8b3ddd4f63604e63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_48be64dd9497468f83d73bd119591271","IPY_MODEL_04b2b191f387469facbc7e0f63edd957","IPY_MODEL_e225b3758fa24df3a0d6f1a039d3220a"],"layout":"IPY_MODEL_aeaed97ed3f441e9aa2ce24c87e02d87"}},"c96a1b051a7b4fbfbd873be07cf44cf0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb886b4dac084c0290e1fd1c229b092e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4facca9ecbd74aa5b4dc474634686064","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f52b2088b6724e6dad9ee18ba364c009","value":1}},"cebd82bbc195424a908c9527ee1a21d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1f94d67f08449439e3191bcdf87c6bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac217ebd99d94729ac89ed81fc0a0ab5","placeholder":"​","style":"IPY_MODEL_2b25549d8eac4efd99bf1beb4fb26b0c","value":"Extracting data files: 100%"}},"de38e0a8f5a24cbdbf755db3cfd399ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_823cdbf0fa2c43559d01de4664258a86","placeholder":"​","style":"IPY_MODEL_e5ae38c7214c4f05974de99e5d5c3485","value":" 499/0 [00:00&lt;00:00, 2393.49 examples/s]"}},"df0a135d8a5b43d5ab94bef15b2db5aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0ceffacff7f492d87084da291061006","max":5942065440,"min":0,"orientation":"horizontal","style":"IPY_MODEL_af87959da48a436e842f58ac691717df","value":5942065440}},"e225b3758fa24df3a0d6f1a039d3220a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c4a2676871e492897d305d6d9a6fac9","placeholder":"​","style":"IPY_MODEL_f432e32a03704652a5bcd21c7ce36abd","value":" 1/1 [00:00&lt;00:00, 29.62it/s]"}},"e35a5293e19748679095d1222f1a31e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5ae38c7214c4f05974de99e5d5c3485":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb30e73c1e824fa8942f0c58104d696f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c96a1b051a7b4fbfbd873be07cf44cf0","placeholder":"​","style":"IPY_MODEL_fa37a3f2205749468f31309b6061ffef","value":"mistral-7b-instruct-v0.2.Q6_K.gguf: 100%"}},"f432e32a03704652a5bcd21c7ce36abd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f52b2088b6724e6dad9ee18ba364c009":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fa37a3f2205749468f31309b6061ffef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa5555299e2e47ae9d2cc7a7e58415f4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fca89659d3684477bb46613bbb96383d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8224768,"sourceType":"datasetVersion","datasetId":4873203}],"dockerImageVersionId":30703,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Installing dependencies. You might need to tweak the CMAKE_ARGS for the `llama-cpp-python` pip package.","metadata":{}},{"cell_type":"markdown","source":"# Installing Libraries","metadata":{}},{"cell_type":"code","source":"# !echo $CUDA_HOME\n!export PATH=\"/usr/local/cuda/bin:$PATH\"\n# !nvidia-smi\n# !cuda-toolkit --version","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:42:07.138784Z","iopub.execute_input":"2024-04-25T09:42:07.139169Z","iopub.status.idle":"2024-04-25T09:42:08.140393Z","shell.execute_reply.started":"2024-04-25T09:42:07.139136Z","shell.execute_reply":"2024-04-25T09:42:08.139157Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# For download the models\n!pip install huggingface_hub -qU\n!pip install datasets -qU","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:34:08.671191Z","iopub.execute_input":"2024-04-25T09:34:08.671580Z","iopub.status.idle":"2024-04-25T09:34:34.850392Z","shell.execute_reply.started":"2024-04-25T09:34:08.671546Z","shell.execute_reply":"2024-04-25T09:34:34.848976Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"/usr/local/cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"# GPU llama-cpp-python; Starting from version llama-cpp-python==0.1.79, it supports GGUF\n!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir -qU\n# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on \" pip install 'llama-cpp-python>=0.1.79' --force-reinstall --upgrade --no-cache-dir -q","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CKL68Itp9Bm-","outputId":"dd33c010-aa3e-4f6a-c763-e30047591c5e","execution":{"iopub.status.busy":"2024-04-25T09:43:41.266823Z","iopub.execute_input":"2024-04-25T09:43:41.268159Z","iopub.status.idle":"2024-04-25T09:44:12.634043Z","shell.execute_reply.started":"2024-04-25T09:43:41.268098Z","shell.execute_reply":"2024-04-25T09:44:12.632905Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for llama-cpp-python \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m \u001b[31m[49 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m \u001b[92m***\u001b[0m \u001b[1m\u001b[92mscikit-build-core 0.9.2\u001b[0m using \u001b[94mCMake 3.29.2\u001b[0m \u001b[91m(wheel)\u001b[0m\u001b[0m\n  \u001b[31m   \u001b[0m \u001b[92m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n  \u001b[31m   \u001b[0m 2024-04-25 09:44:11,297 - scikit_build_core - WARNING - libdir/ldlibrary: /opt/conda/lib/libpython3.10.a is not a real file!\n  \u001b[31m   \u001b[0m 2024-04-25 09:44:11,297 - scikit_build_core - WARNING - Can't find a Python library, got libdir=/opt/conda/lib, ldlibrary=libpython3.10.a, multiarch=x86_64-linux-gnu, masd=None\n  \u001b[31m   \u001b[0m loading initial cache file /tmp/tmpntnomri_/build/CMakeInit.txt\n  \u001b[31m   \u001b[0m -- The C compiler identification is GNU 9.4.0\n  \u001b[31m   \u001b[0m -- The CXX compiler identification is GNU 9.4.0\n  \u001b[31m   \u001b[0m -- Detecting C compiler ABI info\n  \u001b[31m   \u001b[0m -- Detecting C compiler ABI info - done\n  \u001b[31m   \u001b[0m -- Check for working C compiler: /usr/bin/cc - skipped\n  \u001b[31m   \u001b[0m -- Detecting C compile features\n  \u001b[31m   \u001b[0m -- Detecting C compile features - done\n  \u001b[31m   \u001b[0m -- Detecting CXX compiler ABI info\n  \u001b[31m   \u001b[0m -- Detecting CXX compiler ABI info - done\n  \u001b[31m   \u001b[0m -- Check for working CXX compiler: /usr/bin/c++ - skipped\n  \u001b[31m   \u001b[0m -- Detecting CXX compile features\n  \u001b[31m   \u001b[0m -- Detecting CXX compile features - done\n  \u001b[31m   \u001b[0m -- Found Git: /usr/bin/git (found version \"2.25.1\")\n  \u001b[31m   \u001b[0m -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n  \u001b[31m   \u001b[0m -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n  \u001b[31m   \u001b[0m -- Check if compiler accepts -pthread\n  \u001b[31m   \u001b[0m -- Check if compiler accepts -pthread - yes\n  \u001b[31m   \u001b[0m -- Found Threads: TRUE\n  \u001b[31m   \u001b[0m -- Unable to find cudart library.\n  \u001b[31m   \u001b[0m -- Could NOT find CUDAToolkit (missing: CUDA_CUDART) (found version \"12.1.105\")\n  \u001b[31m   \u001b[0m \u001b[33mCMake Warning at vendor/llama.cpp/CMakeLists.txt:458 (message):\n  \u001b[31m   \u001b[0m   CUDA not found\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m \u001b[0m\n  \u001b[31m   \u001b[0m \u001b[0m-- CUDA host compiler is GNU \u001b[0m\n  \u001b[31m   \u001b[0m \u001b[31mCMake Error at vendor/llama.cpp/CMakeLists.txt:903 (get_flags):\n  \u001b[31m   \u001b[0m   get_flags Function invoked with incorrect arguments for function named:\n  \u001b[31m   \u001b[0m   get_flags\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m \u001b[0m\n  \u001b[31m   \u001b[0m -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF\n  \u001b[31m   \u001b[0m -- CMAKE_SYSTEM_PROCESSOR: x86_64\n  \u001b[31m   \u001b[0m -- x86 detected\n  \u001b[31m   \u001b[0m \u001b[33mCMake Warning (dev) at CMakeLists.txt:26 (install):\n  \u001b[31m   \u001b[0m   Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n  \u001b[31m   \u001b[0m This warning is for project developers.  Use -Wno-dev to suppress it.\n  \u001b[31m   \u001b[0m \u001b[0m\n  \u001b[31m   \u001b[0m \u001b[33mCMake Warning (dev) at CMakeLists.txt:35 (install):\n  \u001b[31m   \u001b[0m   Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n  \u001b[31m   \u001b[0m This warning is for project developers.  Use -Wno-dev to suppress it.\n  \u001b[31m   \u001b[0m \u001b[0m\n  \u001b[31m   \u001b[0m -- Configuring incomplete, errors occurred!\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m \u001b[91m\u001b[1m*** CMake configuration failed\u001b[0m\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[31m  ERROR: Failed building wheel for llama-cpp-python\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: Could not build wheels for llama-cpp-python, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install llama_cpp_python -qU","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:16:16.862809Z","iopub.execute_input":"2024-04-25T09:16:16.863186Z","iopub.status.idle":"2024-04-25T09:17:46.100603Z","shell.execute_reply.started":"2024-04-25T09:16:16.863152Z","shell.execute_reply":"2024-04-25T09:17:46.099528Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Loading Model","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\n\n# model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n# model_basename = \"mistral-7b-instruct-v0.2.Q6_K.gguf\"\nmodel_name_or_path = \"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF\"\nmodel_basename = \"Meta-Llama-3-8B-Instruct.Q2_K.gguf\"\nmodel_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:22:37.651522Z","iopub.execute_input":"2024-04-25T09:22:37.652336Z","iopub.status.idle":"2024-04-25T09:22:37.794509Z","shell.execute_reply.started":"2024-04-25T09:22:37.652303Z","shell.execute_reply":"2024-04-25T09:22:37.793532Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# This config has been tested on an RTX 3080 (VRAM of 16GB). You might need to tweak with respect to your hardware.\nfrom llama_cpp import Llama\nlcpp_llm = Llama(\n    model_path=model_path,\n    n_threads=16, # CPU cores\n    n_batch=8000, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n    n_gpu_layers=32, # Change this value based on your model and your GPU VRAM pool.\n    n_ctx=8192, # Context window\n    logits_all=True,\n    verbose=False\n)\n\nrun_on_test = False # whether this baseline system is ran on the test splits or the val splits","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106,"referenced_widgets":["2ae89d1a8a074a249b750d138587e44d","eb30e73c1e824fa8942f0c58104d696f","df0a135d8a5b43d5ab94bef15b2db5aa","a5e99c0d3739407799fde2f29a301d05","fa5555299e2e47ae9d2cc7a7e58415f4","c96a1b051a7b4fbfbd873be07cf44cf0","fa37a3f2205749468f31309b6061ffef","a0ceffacff7f492d87084da291061006","af87959da48a436e842f58ac691717df","e35a5293e19748679095d1222f1a31e5","2abefc6082af406ab1c955a880a2b419"]},"id":"uDMqQmBfAhYO","outputId":"eacd2078-6e5a-4451-84b4-69c6789cb4d1","execution":{"iopub.status.busy":"2024-04-25T09:31:24.915807Z","iopub.execute_input":"2024-04-25T09:31:24.916448Z","iopub.status.idle":"2024-04-25T09:31:26.698812Z","shell.execute_reply.started":"2024-04-25T09:31:24.916406Z","shell.execute_reply":"2024-04-25T09:31:26.697753Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Running the model","metadata":{}},{"cell_type":"code","source":"import os\nprint(os.listdir(\"/kaggle/input/\"))\n\nif not os.path.exists('/kaggle/working/output/'):\n    os.mkdir('/kaggle/working/output/')","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:26:47.024743Z","iopub.execute_input":"2024-04-25T09:26:47.025526Z","iopub.status.idle":"2024-04-25T09:26:47.030327Z","shell.execute_reply.started":"2024-04-25T09:26:47.025496Z","shell.execute_reply":"2024-04-25T09:26:47.029309Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"MODEL_NAME = model_name_or_path.split('/')[1]\n# make sure the output filename is the same as the reference filename for the scoring program\npath_val_model_agnostic = \"/kaggle/input/nlp-project-dataset/data/val/val.model-agnostic.json\"\npath_val_model_agnostic_output = f\"/kaggle/working/output/{MODEL_NAME}/val.model-agnostic.json\"\n\nfrom datasets import load_dataset\nimport json\nimport random\nimport numpy as np\nimport tqdm.notebook as tqdm\nseed_val = 442\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\n\n# alternatively, one can use HuggingFace's library to load the data\ndataset = load_dataset('json', data_files={'val':path_val_model_agnostic})\ndata_val_all = dataset['val']\nnum_sample = len(data_val_all)\nprint(dataset)\nprint(num_sample)\n\noutput_json = []\nlabels = [\"Not Hallucination\", \"Hallucination\"]\n\"\"\"\nSelfCheckGPT Usage: (LLM) Prompt\nhttps://github.com/potsawee/selfcheckgpt\nContext: {}\nSentence: {}\nIs the sentence supported by the context above?\nAnswer Yes or No:\n\"\"\"\nfor i in tqdm.trange(num_sample):\n    # label = str(data_val_all[i]['label'])\n    task = str(data_val_all[i]['task'])\n    if run_on_test:\n        # test splits will contain ids to ensure correct alignment before scoring\n        id = int(data_val_all[i]['id'])\n    hyp = str(data_val_all[i]['hyp'])\n    src = str(data_val_all[i]['src'])\n    tgt = str(data_val_all[i]['tgt'])\n\n    if task == \"PG\":\n        context = f\"Context: {src}\"\n    else: #i.e. task == \"MT\" or task == \"DM\":\n        context = f\"Context: {tgt}\"\n\n    sentence = f\"Sentence: {hyp}\"\n    message = f\"{context}\\n{sentence}\\nIs the Sentence supported by the Context above? Answer using ONLY yes or no:\"\n    prompt = f\"<s>[INST] {message} [/INST]\"\n\n    response = lcpp_llm(\n        prompt=prompt,\n        temperature= 0.0,\n        logprobs=1,\n    )\n    answer = str(response[\"choices\"][0][\"text\"]).strip().lower()\n    if answer.startswith(\"yes\"):\n        output_label = \"Not Hallucination\"\n        prob = 1-float(np.exp(response[\"choices\"][0][\"logprobs\"][\"token_logprobs\"][0]))\n    if answer.startswith(\"no\"):\n        output_label = \"Hallucination\"\n        prob = float(np.exp(response[\"choices\"][0][\"logprobs\"][\"token_logprobs\"][0]))\n    if not answer.startswith(\"no\") and not answer.startswith(\"yes\"):\n        idx_random = random.randint(0,len(labels)-1)\n        output_label = labels[idx_random]\n        prob = float(0.5)\n\n    item_to_json = {\"label\":output_label, \"p(Hallucination)\":prob}\n    if run_on_test:\n        item_to_json['id'] = id\n    output_json.append(item_to_json)\n\n\nf = open(path_val_model_agnostic_output, 'w', encoding='utf-8')\njson.dump(output_json, f)\nf.close()\nprint(\"done\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["aff193ecfc2e4d5a8b3ddd4f63604e63","48be64dd9497468f83d73bd119591271","04b2b191f387469facbc7e0f63edd957","e225b3758fa24df3a0d6f1a039d3220a","aeaed97ed3f441e9aa2ce24c87e02d87","cebd82bbc195424a908c9527ee1a21d3","8665cfefbc984fc4873e73cd96d6c018","1c18583fabf94cf88d89e9d0ad83cd46","16ceb8ceabea4adeb2ed5d3c62a52e87","6c4a2676871e492897d305d6d9a6fac9","f432e32a03704652a5bcd21c7ce36abd","86da540e05824f2c95b5c8bea9b4581d","d1f94d67f08449439e3191bcdf87c6bf","cb886b4dac084c0290e1fd1c229b092e","8b8fd80c79c54e479b15f798bc545b96","3e1566a3d2f64b5fbbaf7cc51b9c9902","ac217ebd99d94729ac89ed81fc0a0ab5","2b25549d8eac4efd99bf1beb4fb26b0c","4facca9ecbd74aa5b4dc474634686064","f52b2088b6724e6dad9ee18ba364c009","08db236b9ee74ccb9ac456bf09e298e1","977e8b1928ec42a285804dcc8fc13cb5","a0f2fe09ab0a4a21acda513f96bb7faf","4f891d2316604dd08cd5ffd22c8854d9","0ea36c0ff6cd4559bf733fb73ff82693","de38e0a8f5a24cbdbf755db3cfd399ec","9f4e1bc76cfb4643877686a6f0271b52","5c70248a7e6e45199ed626fa68037174","07bb3c8d23084467b680d0f8be879bcd","fca89659d3684477bb46613bbb96383d","265b13864e334d2d8875d1de157c428a","823cdbf0fa2c43559d01de4664258a86","e5ae38c7214c4f05974de99e5d5c3485"]},"id":"-2KYuv-H-LYU","outputId":"55d8a874-ee9c-4833-f426-279caf6813ec","execution":{"iopub.status.busy":"2024-04-25T09:31:34.226817Z","iopub.execute_input":"2024-04-25T09:31:34.227544Z","iopub.status.idle":"2024-04-25T09:32:20.632319Z","shell.execute_reply.started":"2024-04-25T09:31:34.227514Z","shell.execute_reply":"2024-04-25T09:32:20.630719Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"DatasetDict({\n    val: Dataset({\n        features: ['hyp', 'p(Hallucination)', 'ref', 'labels', 'tgt', 'task', 'label', 'src', 'model'],\n        num_rows: 499\n    })\n})\n499\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/499 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b034d429ea304369904a8f7603607151"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIs the Sentence supported by the Context above? Answer using ONLY yes or no:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<s>[INST] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [/INST]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 51\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mlcpp_llm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:1561\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1498\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1499\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1523\u001b[0m     logit_bias: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1524\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[1;32m   1526\u001b[0m \n\u001b[1;32m   1527\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;124;03m        Response object containing the generated text.\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mecho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:1494\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1492\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1494\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion_or_chunks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:1019\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1017\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1019\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m   1020\u001b[0m     prompt_tokens,\n\u001b[1;32m   1021\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1022\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   1023\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[1;32m   1024\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[1;32m   1025\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m   1026\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[1;32m   1027\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1028\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1029\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1030\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1031\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1032\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1033\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1034\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m   1035\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m   1036\u001b[0m ):\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llama_cpp\u001b[38;5;241m.\u001b[39mllama_token_is_eog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mmodel, token):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:698\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 698\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[1;32m    700\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    701\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    702\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[1;32m    717\u001b[0m         )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:536\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    532\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[1;32m    534\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[1;32m    535\u001b[0m )\n\u001b[0;32m--> 536\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llama_cpp/_internals.py:311\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}